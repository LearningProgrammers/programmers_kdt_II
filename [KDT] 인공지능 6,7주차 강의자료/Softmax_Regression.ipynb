{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제\n",
    "\n",
    "- L2 regularization을 비용함수(compute_cost 내에)에 포함시키고 gradient 계산에(batch_gd 내에) 반영하세요.\n",
    "\n",
    "- Regularization을 위한 가중치 lambda를 튜닝해보세요. 이것을 위해서 학습데이터의 일부를 validation data로 따로 구분하고 이 validation data에 대한 accuracy를 최적화하는 lambda를 찾도록 하는 코드를 구현해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist[\"data\"], mnist[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-78fc683d5810>:1: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  enc.fit(y[:,np.newaxis])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneHotEncoder()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.fit(y[:,np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-fd377ab0d802>:1: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  Y = enc.transform(y[:,np.newaxis]).toarray()\n"
     ]
    }
   ],
   "source": [
    "Y = enc.transform(y[:,np.newaxis]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], Y[:60000], Y[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, W):\n",
    "    K = np.size(W, 1)\n",
    "    A = np.exp(X @ W)\n",
    "    B = np.diag(1 / (np.reshape(A @ np.ones((K,1)), -1)))\n",
    "    Y = B @ A\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, T, W, L=0):\n",
    "    epsilon = 1e-5\n",
    "    N = len(T)\n",
    "    K = np.size(T, 1)\n",
    "    cost = - (1/N) * np.ones((1,N)) @ (np.multiply(np.log(softmax(X, W) + epsilon), T)) @ np.ones((K,1))\n",
    "    l2_reg = L * np.sum(np.square(W)) / (2 * N)  # L2 regularization\n",
    "    return cost + l2_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W):\n",
    "    return np.argmax((X @ W), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gd(X, T, W, learning_rate, iterations, batch_size, L=0, random_state=None, output=True):\n",
    "    N = len(T)\n",
    "    cost_history = np.zeros((iterations,1))\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    shuffled_indices = np.random.permutation(N)\n",
    "    X_shuffled = X[shuffled_indices]\n",
    "    T_shuffled = T[shuffled_indices]\n",
    "\n",
    "    #batch gradient descent\n",
    "    for i in range(iterations):\n",
    "        j = i % N\n",
    "        X_batch = X_shuffled[j:j+batch_size]\n",
    "        T_batch = T_shuffled[j:j+batch_size]\n",
    "        # batch가 epoch 경계를 넘어가는 경우, 앞 부분으로 채워줌\n",
    "        if X_batch.shape[0] < batch_size:\n",
    "            X_batch = np.vstack((X_batch, X_shuffled[:(batch_size - X_batch.shape[0])]))\n",
    "            T_batch = np.vstack((T_batch, T_shuffled[:(batch_size - T_batch.shape[0])]))\n",
    "        # W = W - (learning_rate/batch_size) * (X_batch.T @ (softmax(X_batch, W) - T_batch))\n",
    "        W = W * (1 - learning_rate * L / batch_size) \\\n",
    "                - (learning_rate/batch_size) * (X_batch.T @ (softmax(X_batch, W) - T_batch)) # 동시에 모든 w가 업데이트된다.\n",
    "        cost_history[i] = compute_cost(X_batch, T_batch, W, L)\n",
    "        if output and i % 1000 == 0:\n",
    "            print(cost_history[i][0])\n",
    "\n",
    "    return (cost_history, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost is: 2.3024850979937352 \n",
      "\n",
      "2.266748970882968\n",
      "0.5400978467531193\n",
      "0.49350895972261355\n",
      "0.6335910549286129\n",
      "0.5179386903190486\n",
      "0.39343459293109573\n",
      "0.27158959072596006\n",
      "0.34270031343106944\n",
      "0.3392035463570107\n",
      "0.20529268551273083\n",
      "0.4437027700265992\n",
      "0.29894261067380845\n",
      "0.40433897856135026\n",
      "0.19009442209743127\n",
      "0.20566270247645058\n",
      "0.2740145117348238\n",
      "0.31500076889654005\n",
      "0.31185340483735846\n",
      "0.3623704128575332\n",
      "0.2544591524695475\n",
      "0.2485276643579266\n",
      "0.18499519326938618\n",
      "0.18137318227847146\n",
      "0.30263807229628026\n",
      "0.195708340245924\n",
      "0.20684270932558046\n",
      "0.27570650053396606\n",
      "0.2159668002074422\n",
      "0.30631827256073485\n",
      "0.19851437911668196\n",
      "0.21886011688795737\n",
      "0.17155665320261954\n",
      "0.3121909882427808\n",
      "0.27075991900651103\n",
      "0.2692022032336735\n",
      "0.4534745516145777\n",
      "0.15715881610349342\n",
      "0.4883679996264292\n",
      "0.3759676611219625\n",
      "0.299828403462353\n",
      "0.18024565596879205\n",
      "0.26687640991605915\n",
      "0.26408092228484176\n",
      "0.3896181832296417\n",
      "0.21708487574307514\n",
      "0.2059124875219347\n",
      "0.3743579788422275\n",
      "0.11082518020745857\n",
      "0.14860297843744769\n",
      "0.15720335499662788\n"
     ]
    }
   ],
   "source": [
    "# 규제화 전\n",
    "X = np.hstack((np.ones((np.size(X_train, 0),1)),X_train))\n",
    "T = y_train\n",
    "\n",
    "K = np.size(T, 1)\n",
    "M = np.size(X, 1)\n",
    "W = np.zeros((M,K))\n",
    "\n",
    "iterations = 50000\n",
    "learning_rate = 0.01\n",
    "\n",
    "initial_cost = compute_cost(X, T, W)\n",
    "\n",
    "print(\"Initial Cost is: {} \\n\".format(initial_cost[0][0]))\n",
    "\n",
    "(cost_history, W_optimal) = batch_gd(X, T, W, learning_rate, iterations, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost is: 2.3024850979937352 \n",
      "\n",
      "2.281596456462674\n",
      "0.45036031171500956\n",
      "0.36421365194549976\n",
      "0.45171165106043915\n",
      "0.3796137362628895\n",
      "0.5665716677800248\n",
      "0.40514578721139827\n",
      "0.44531476824016913\n",
      "0.2415916442078665\n",
      "0.3163976990316153\n",
      "0.20979593966929683\n",
      "0.18360640740087217\n",
      "0.346179196019251\n",
      "0.08094083981645207\n",
      "0.23250496316265254\n",
      "0.264800530807613\n",
      "0.4316783702506057\n",
      "0.29172080452456545\n",
      "0.4710830201259775\n",
      "0.14807698092735447\n",
      "0.36935958323104895\n",
      "0.5040785906007762\n",
      "0.409769368401717\n",
      "0.2671081658786581\n",
      "0.35456603712789153\n",
      "0.12317469962365907\n",
      "0.2371393397282384\n",
      "0.40803480743724563\n",
      "0.20909513086710643\n",
      "0.1720757437102052\n",
      "0.2512778325244281\n",
      "0.16947396928572217\n",
      "0.30270770732859853\n",
      "0.2883264480423924\n",
      "0.1982483400832251\n",
      "0.4467052632141238\n",
      "0.26591797168399206\n",
      "0.13453931380247425\n",
      "0.41467851604277006\n",
      "0.31235410058681606\n",
      "0.16491264792151467\n",
      "0.3037857601653567\n",
      "0.2456135098477715\n",
      "0.3367021448088752\n",
      "0.3950400462809416\n",
      "0.2527336384044906\n",
      "0.3201201464768421\n",
      "0.17783799868556752\n",
      "0.19666759147727966\n",
      "0.2848086359913297\n"
     ]
    }
   ],
   "source": [
    "# 규제화\n",
    "X = np.hstack((np.ones((np.size(X_train, 0),1)),X_train))\n",
    "T = y_train\n",
    "\n",
    "K = np.size(T, 1)\n",
    "M = np.size(X, 1)\n",
    "W = np.zeros((M,K))\n",
    "\n",
    "iterations = 50000\n",
    "learning_rate = 0.01\n",
    "L = 0.01\n",
    "\n",
    "initial_cost = compute_cost(X, T, W, L)\n",
    "\n",
    "print(\"Initial Cost is: {} \\n\".format(initial_cost[0][0]))\n",
    "\n",
    "(cost_history_with_reg, W_reg_optimal) = batch_gd(X, T, W, learning_rate, iterations, 64, L, random_state=210613)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(y_targ, y_pred):\n",
    "    return (y_targ == y_pred).sum()/ len(y_targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9174 0.9155\n"
     ]
    }
   ],
   "source": [
    "## Accuracy\n",
    "X_ = np.hstack((np.ones((np.size(X_test, 0),1)),X_test))\n",
    "T_ = y_test\n",
    "y_pred = predict(X_, W_optimal)\n",
    "y_reg_pred = predict(X_, W_reg_optimal)\n",
    "y_targ = np.argmax(T_, axis=1)\n",
    "\n",
    "# score = float(sum(y_pred == np.argmax(T_, axis=1)))/ float(len(y_test))\n",
    "score1 = accuray_score(y_targ, y_pred)\n",
    "score2 = accuray_score(y_targ, y_reg_pred)\n",
    "\n",
    "print(score1, score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(X, Y, valid_data_ratio=0.2, random_state=None):\n",
    "    if random_state is not None: # random_state가 변경되지 않는다면 같은 난수발생시키기 위함\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    N = len(X)\n",
    "    val_size = int(N * valid_data_ratio)\n",
    "    train_size = N - val_size\n",
    "    \n",
    "    train_idx = np.random.choice(N, train_size, replace=False)\n",
    "    val_idx = np.setdiff1d(np.arange(N), train_idx)\n",
    "    \n",
    "    return X[train_idx], X[val_idx], Y[train_idx], Y[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda = 0.0, accuracy = 0.8718333333333333\n",
      "lambda = 0.0001, accuracy = 0.8718333333333333\n",
      "lambda = 0.0001438449888287663, accuracy = 0.8718333333333333\n",
      "lambda = 0.00020691380811147902, accuracy = 0.8718333333333333\n",
      "lambda = 0.00029763514416313193, accuracy = 0.8718333333333333\n",
      "lambda = 0.00042813323987193956, accuracy = 0.8718333333333333\n",
      "lambda = 0.0006158482110660267, accuracy = 0.8718333333333333\n",
      "lambda = 0.0008858667904100823, accuracy = 0.8718333333333333\n",
      "lambda = 0.0012742749857031334, accuracy = 0.87175\n",
      "lambda = 0.0018329807108324356, accuracy = 0.87175\n",
      "lambda = 0.0026366508987303583, accuracy = 0.87175\n",
      "lambda = 0.00379269019073225, accuracy = 0.87175\n",
      "lambda = 0.005455594781168515, accuracy = 0.8718333333333333\n",
      "lambda = 0.007847599703514606, accuracy = 0.8718333333333333\n",
      "lambda = 0.011288378916846883, accuracy = 0.8718333333333333\n",
      "lambda = 0.01623776739188721, accuracy = 0.87175\n",
      "lambda = 0.023357214690901212, accuracy = 0.87175\n",
      "lambda = 0.03359818286283781, accuracy = 0.8716666666666667\n",
      "lambda = 0.04832930238571752, accuracy = 0.8715\n",
      "lambda = 0.06951927961775606, accuracy = 0.87125\n",
      "lambda = 0.1, accuracy = 0.8705833333333334\n",
      "\n",
      "max_lambda = 0.0, max_score = 0.8718333333333333\n"
     ]
    }
   ],
   "source": [
    "# 최적화 lambda 찾기\n",
    "X = np.hstack((np.ones((np.size(X_train, 0),1)),X_train))\n",
    "T = y_train\n",
    "\n",
    "X_train2, X_val, y_train2, y_val = train_val_split(X, T, random_state=210613)\n",
    "y_targ = np.argmax(y_val, axis=1)\n",
    "\n",
    "K = np.size(T, 1)\n",
    "M = np.size(X, 1)\n",
    "W = np.zeros((M, K))\n",
    "\n",
    "iterations = 5000\n",
    "learning_rate = 0.01\n",
    "\n",
    "lambdas = np.logspace(-4, -1, 20)\n",
    "lambdas = np.insert(lambdas, 0, 0)\n",
    "result = []\n",
    "\n",
    "for L in lambdas:\n",
    "    _, W_opt = batch_gd(X_train2, y_train2, W, learning_rate, iterations, 64, L, random_state=210613, output=False)\n",
    "    \n",
    "    y_pred = predict(X_val, W_opt)\n",
    "    score = accuracy_score(y_targ, y_pred)\n",
    "    result.append([L, score])\n",
    "    print(f\"lambda = {L}, accuracy = {score}\")\n",
    "\n",
    "max_lambda, max_score = max(result, key=lambda x: x[1])\n",
    "\n",
    "print()\n",
    "print(f\"{max_lambda = }, {max_score = }\")  # python >= 3.8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
